{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e312bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b09cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from common import d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa5c7d9",
   "metadata": {},
   "source": [
    "#### 1. 填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "549f346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    return Y.reshape(Y.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9a178a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70722ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7956, 0.8159, 0.0415, 0.4644, 0.6488, 0.4344, 0.9109, 0.6410],\n",
       "        [0.4336, 0.5314, 0.7623, 0.8971, 0.5424, 0.1760, 0.2694, 0.9884],\n",
       "        [0.4103, 0.9388, 0.6596, 0.3007, 0.0743, 0.8236, 0.1522, 0.1374],\n",
       "        [0.9271, 0.3509, 0.5246, 0.1063, 0.7034, 0.1629, 0.3682, 0.7009],\n",
       "        [0.4503, 0.9445, 0.3662, 0.9807, 0.8342, 0.6674, 0.3182, 0.4974],\n",
       "        [0.1113, 0.1155, 0.7469, 0.3005, 0.4464, 0.6874, 0.9967, 0.8691],\n",
       "        [0.5218, 0.9945, 0.1506, 0.9075, 0.9784, 0.8591, 0.5273, 0.7428],\n",
       "        [0.5603, 0.5415, 0.5951, 0.9339, 0.0880, 0.7482, 0.5135, 0.3894]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(8, 8))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fefc98ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5588, -0.3456, -0.2963, -0.6316, -0.5385, -0.4415, -0.4611, -0.3701],\n",
       "         [-0.7734, -0.5445, -0.6773, -0.7851, -0.3333, -0.5348, -0.7926, -0.2839],\n",
       "         [-0.8333, -0.8691, -0.6657, -0.3405, -0.3753, -0.4691, -0.4598, -0.4109],\n",
       "         [-0.8442, -0.6970, -0.3943, -0.3629, -0.8079, -0.5375, -0.3799, -0.3681],\n",
       "         [-0.7198, -0.4124, -0.4837, -0.7885, -0.5763, -0.3882, -0.7187, -0.5384],\n",
       "         [-0.6037, -0.7306, -0.6767, -0.6865, -0.7693, -0.7722, -0.7362, -0.4198],\n",
       "         [-0.6095, -0.7248, -0.5604, -0.8450, -0.7853, -0.7856, -0.7852, -0.3042],\n",
       "         [-0.7887, -0.4031, -0.5176, -0.7174, -0.4220, -0.5127, -0.3748, -0.1447]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " torch.Size([8, 8]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = comp_conv2d(conv2d, X)\n",
    "Y, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5450b5",
   "metadata": {},
   "source": [
    "#### 2. 步幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "371e6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "182b2280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1331, 0.4155, 0.2503, 0.3474],\n",
       "         [0.0721, 0.2925, 0.2184, 0.5206],\n",
       "         [0.1040, 0.0971, 0.2541, 0.4782],\n",
       "         [0.1407, 0.4404, 0.5819, 0.4704]], grad_fn=<ViewBackward0>),\n",
       " torch.Size([4, 4]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = comp_conv2d(conv2d, X)\n",
    "Y, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c061e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9f06b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1324, -0.2481],\n",
       "         [-0.2125, -0.3611]], grad_fn=<ViewBackward0>),\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = comp_conv2d(conv2d, X)\n",
    "Y, Y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
