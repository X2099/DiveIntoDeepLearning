好的，以下是更新后的 **3.2 文档加载与预处理** 小节，加入了**PDF 文档读取的示例**，并保持与整体风格一致：

---

### 3.2 文档加载与预处理

在构建 Agent 之前，我们需要先将原始文档转化为可以检索的向量形式，具体包括：

#### （1）加载本地文档

LangChain 提供多种 `Loader` 来读取不同格式的文档。

* **读取 Markdown 文件**（`.md`）：

```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader('./docs/智能体纪元第1篇.md', encoding='utf-8')
documents = loader.load()
```

* **读取整个目录下的 Markdown 文件**：

```python
from langchain_community.document_loaders import DirectoryLoader

loader = DirectoryLoader('./docs', glob='**/*.md', loader_cls=TextLoader)
documents = loader.load()
```

* **读取 PDF 文件**：

对于 PDF 文件，可以使用 `PyPDFLoader` 进行加载。它会按页读取并自动转为 `Document` 对象。

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader('./docs/智能体纪元白皮书.pdf')
documents = loader.load()
```

如果你希望将多个来源的文档合并处理，只需拼接多个 `documents` 列表即可：

```python
all_documents = md_documents + pdf_documents + other_documents
```

#### （2）文档切分

为了让长文档适配向量数据库，我们需要按段落或语义块进行切分。推荐使用 `RecursiveCharacterTextSplitter`：

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", "。", "！", "？", ".", "!", "?"]
)

split_docs = text_splitter.split_documents(documents)
```

此操作会将文档拆成多个短文本块，既保留上下文连续性，又便于后续的向量化处理。

---

是否需要我顺带把这部分内容改写成独立函数以便复用？例如 `load_documents_from_dir(path)` 这种结构？
