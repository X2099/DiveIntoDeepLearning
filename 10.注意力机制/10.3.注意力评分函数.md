缩放点积注意力（Scaled Dot-Product Attention）的核心计算公式如下：

$$
\text{Attention}(Q, K, V) = \operatorname{softmax}\!\Bigl(\frac{QK^{T}}{\sqrt{d_k}}\Bigr)\,V,
$$

其中：

- \(Q\in\mathbb{R}^{n\times d_k}\) 是查询矩阵（queries），  
- \(K\in\mathbb{R}^{m\times d_k}\) 是键矩阵（keys），  
- \(V\in\mathbb{R}^{m\times d_v}\) 是值矩阵（values），  
- \(d_k\) 是键向量的维度，  
- \(\operatorname{softmax}\) 操作对每一行进行归一化，保证权重和为 1。

完整的 LaTeX 源码如下，方便复制使用：

```latex
\[
\text{Attention}(Q, K, V) = \operatorname{softmax}\!\Bigl(\frac{QK^{T}}{\sqrt{d_k}}\Bigr)\,V,
\]
```

如果需要在行内使用，也可写为：  
```latex
$\text{Attention}(Q, K, V) = \operatorname{softmax}\bigl(\frac{QK^{T}}{\sqrt{d_k}}\bigr)\,V$
```