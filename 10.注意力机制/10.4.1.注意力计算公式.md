## 3.2 注意力计算公式详解：逐层拆解智能关注机制

让我们像拆解手机零件一样，仔细分析注意力计算的每个组件，理解这个"智能聚光灯"的工作原理。

### 3.2.1 计算流程图解
![注意力计算流程图](https://example.com/attention-calculation-flow.png)

### 3.2.2 公式逐参数解析
假设现在要翻译第t个目标词（如法语的第三个词），计算过程如下：

#### 步骤1：计算对齐分数（能量值）
$$e_{ti} = v^T \tanh(W_q q_t + W_k h_i + b)$$

```latex
e_{ti} = v^T \tanh(W_q q_t + W_k h_i + b)
```

- **参数说明**：
  - $q_t$：解码器当前状态（查询向量），形状为$d_q$
    - 相当于问："我现在需要关注源句子的哪个部分？"
    - 示例值：假设$d_q=256$，则$q_t \in \mathbb{R}^{256}$
  
  - $h_i$：编码器第i个位置的隐状态（键/值向量），形状$d_h$
    - 存储源句子第i个位置的信息
    - 示例：$h_{i=3}$表示源句子第三个词的上下文信息
  
  - $W_q \in \mathbb{R}^{d_a \times d_q}$：查询变换矩阵
  - $W_k \in \mathbb{R}^{d_a \times d_h}$：键变换矩阵
  - $b \in \mathbb{R}^{d_a}$：偏置向量
  - $v \in \mathbb{R}^{d_a}$：能量转换向量

- **计算过程**：
  1. 将查询向量和键向量投影到相同空间：
     $$ \text{投影后} = W_q q_t + W_k h_i + b $$
     ```latex
     W_q q_t + W_k h_i + b
     ```
  2. 通过tanh激活函数进行非线性变换
  3. 与向量v做点积，得到标量能量值

- **物理意义**：衡量目标位置t与源位置i的关联强度。就像用探照灯扫描源句子时，能量值越大，该位置越亮。

#### 步骤2：生成注意力权重
$$\alpha_{ti} = \frac{\exp(e_{ti})}{\sum_{j=1}^T \exp(e_{tj})}$$
```latex
\alpha_{ti} = \frac{\exp(e_{ti})}{\sum_{j=1}^T \exp(e_{tj})}
```

- **参数说明**：
  - $T$：源句子长度
  - $\exp$：指数函数，确保值为正数

- **计算特点**：
  - Softmax将能量值转换为概率分布
  - 所有$\alpha_{ti}$满足$\sum_{i=1}^T \alpha_{ti} = 1$

- **物理意义**：形成"注意力分布图"，例如：
  ```
  源句子：The cat sat on the mat
  注意力权重：[0.02, 0.15, 0.7, 0.1, 0.02, 0.01]
  表示模型在翻译当前词时，73%的注意力集中在"sat"上
  ```

#### 步骤3：生成上下文向量
$$c_t = \sum_{i=1}^T \alpha_{ti} h_i$$
```latex
c_t = \sum_{i=1}^T \alpha_{ti} h_i
```

- **参数说明**：
  - $h_i$：同步骤1中的编码器隐状态
  - $\alpha_{ti}$：步骤2得到的注意力权重

- **计算特点**：
  - 加权求和过程
  - 结果$c_t$的维度与$h_i$相同

- **物理意义**：生成动态上下文胶囊，例如：
  ```python
  # 假设源句子编码为：
  h = [0.2, 0.5, 0.9, 0.3]  # 四个词的编码
  alpha = [0.1, 0.1, 0.7, 0.1]
  c_t = 0.1*0.2 + 0.1*0.5 + 0.7*0.9 + 0.1*0.3 = 0.73
  ```

### 3.2.3 参数维度示例
假设设定：
- 编码器隐状态维度 $d_h = 512$
- 注意力维度 $d_a = 256$
- 解码器隐状态维度 $d_q = 512$

则各参数矩阵的维度为：
```
W_q ∈ R^{256×512}  
W_k ∈ R^{256×512}  
b ∈ R^{256}  
v ∈ R^{256}
```

矩阵形状验证：
1. $W_q q_t$： (256,512) × (512,1) → (256,1)
2. $W_k h_i$： (256,512) × (512,1) → (256,1)
3. 相加后： (256,1) + (256,1) + (256,1) → (256,1)
4. 经过v^T： (1,256) × (256,1) → 标量值

### 3.2.4 计算实例演示
假设当前要生成法语的第3个词，源句子是英语的"I love machine learning"（4个词）

**参数初始化**：
```python
# 编码器隐状态（简化示例）
h = [
    [0.2, 0.3],  # I
    [0.5, 0.8],  # love
    [0.7, 0.1],  # machine
    [0.4, 0.6]   # learning
]

# 解码器当前状态
q_t = [0.6, 0.4]

# 参数矩阵（简化维度）
W_q = [[0.5, 0.2], [0.3, 0.4]]
W_k = [[0.1, 0.6], [0.5, 0.3]]
b = [0.1, 0.2]
v = [0.5, 0.5]
```

**计算步骤**：
1. 计算第一个词"I"（i=0）的能量值：
   ```
   W_q * q_t = [0.5*0.6 + 0.2*0.4, 0.3*0.6 + 0.4*0.4] = [0.38, 0.34]
   W_k * h0 = [0.1*0.2 + 0.6*0.3, 0.5*0.2 + 0.3*0.3] = [0.20, 0.19]
   相加：0.38+0.20+0.1=0.68, 0.34+0.19+0.2=0.73
   tanh([0.68, 0.73]) ≈ [0.59, 0.62]
   点积：0.5*0.59 + 0.5*0.62 = 0.605
   ```

2. 同样计算其他词的能量值，假设得到：
   ```
   e = [0.605, 0.823, 1.152, 0.417]
   ```

3. Softmax转换：
   ```
   exp_e = [e^0.605≈1.83, e^0.823≈2.28, e^1.152≈3.16, e^0.417≈1.52]
   sum = 1.83 + 2.28 + 3.16 + 1.52 = 8.79
   alpha = [1.83/8.79≈0.208, 2.28/8.79≈0.259, 3.16/8.79≈0.359, 1.52/8.79≈0.173]
   ```

4. 生成上下文向量：
   ```
   c_t = 0.208*[0.2,0.3] + 0.259*[0.5,0.8] + 0.359*[0.7,0.1] + 0.173*[0.4,0.6]
        = [0.208*0.2 + 0.259*0.5 + 0.359*0.7 + 0.173*0.4,
           0.208*0.3 + 0.259*0.8 + 0.359*0.1 + 0.173*0.6]
        ≈ [0.0416 + 0.1295 + 0.2513 + 0.0692, 0.0624 + 0.2072 + 0.0359 + 0.1038]
        ≈ [0.4916, 0.4093]
   ```

### 3.2.5 设计思想解析
- **动态感知**：每个解码步骤重新计算权重，避免传统模型的"记忆模糊"
- **内容寻址**：根据当前需要主动检索相关信息（类似人类回忆机制）
- **可微分**：整个计算过程可导，能通过反向传播自动学习参数
- **计算效率**：时间复杂度为O(T×S)，T为目标长度，S为源长度

通过这种精密的数学设计，模型实现了类似人类的动态注意力分配能力。就像翻译时我们会自然关注源句子的相关部分，Bahdanau注意力通过可学习的参数矩阵，让机器学会了这种智能关注的能力。