# 动手学深度学习：序列到序列学习（seq2seq）详解

## 1. 什么是序列到序列学习？

想象你要把一句英文翻译成法文。输入是一个单词序列（如“They are watching”），输出是另一个长度可能不同的序列（如“Ils regardent”）。**序列到序列学习（seq2seq）** 就是专门处理这类输入输出都是序列的任务的模型。它像两个配合的机器人：一个负责“理解”输入句子（编码器），另一个负责“生成”输出句子（解码器）。

### 核心思想
- **编码器**：将变长的输入序列转换为固定长度的“上下文向量”（好比把一篇文章压缩成摘要）。
- **解码器**：根据上下文向量逐步生成输出序列，每次生成一个词（像根据摘要写读后感）。

![seq2seq结构图](https://zh-v2.d2l.ai/_images/seq2seq.svg)

## 2. 编码器：把句子变成“密码”

### 2.1 工作原理
编码器通常用循环神经网络（RNN）实现。假设输入是“They are watching”，每个单词依次进入RNN：

1. RNN读取“They”，更新内部状态。
2. 读取“are”，再次更新状态。
3. 读取“watching”，得到最终状态$h_3$。

这个最终状态$h_3$就是整个句子的“上下文向量”。数学描述为：

$$h_t = f(x_t, h_{t-1})$$
$$c = h_T$$

其中：
- $h_t$是时间步$t$的隐状态
- $f$是RNN的计算函数（如GRU、LSTM）
- $c$是上下文向量（最后一步的$h_T$）

**LaTeX公式：**
```
$$h_t = f(x_t, h_{t-1})$$
$$c = h_T$$
```

### 2.2 代码示例
```python
class Seq2SeqEncoder:
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers):
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers)
    
    def forward(self, X):
        X = self.embedding(X)       # 转换为词向量
        output, state = self.rnn(X) # 通过RNN层
        return output, state       # 返回最终状态
```

## 3. 解码器：从“密码”生成句子

### 3.1 工作流程
解码器同样用RNN实现，但它的初始状态是编码器的上下文向量$c$。生成过程如下：

1. 初始输入是`<bos>`（序列开始符），状态是$c$。
2. 预测第一个词“Ils”，并更新状态。
3. 以“Ils”作为新输入，预测下一个词“regardent”。
4. 直到输出`<eos>`（结束符）停止。

数学表达式：

$$s_t = g(y_{t-1}, s_{t-1}, c)$$
$$P(y_t | y_{<t}, c) = \text{softmax}(W s_t)$$

**LaTeX公式：**
```
$$s_t = g(y_{t-1}, s_{t-1}, c)$$
$$P(y_t | y_{<t}, c) = \text{softmax}(W s_t)$$
```

### 3.2 代码实现
```python
class Seq2SeqDecoder:
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers):
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers)
        self.dense = nn.Linear(num_hiddens, vocab_size)
    
    def forward(self, X, state):
        X = self.embedding(X)
        context = state[-1].repeat(X.shape[0], 1, 1)  # 扩展上下文
        X_and_context = torch.cat((X, context), 2)    # 拼接输入与上下文
        output, state = self.rnn(X_and_context, state)
        return self.dense(output), state
```

## 4. 损失函数：屏蔽无效位置

### 4.1 问题背景
句子长度不同需要填充（如用`<pad>`补齐）。计算损失时应忽略填充部分。

**示例：**
- 真实标签：`[Ils, regardent, <eos>, <pad>]`
- 预测标签：`[Ils, mangent, <eos>, <pad>]`

只需计算前3个位置的损失。

### 4.2 实现方法
```python
def sequence_mask(X, valid_len):
    maxlen = X.size(1)
    mask = torch.arange(maxlen) < valid_len.unsqueeze(1)
    X[~mask] = 0
    return X

class MaskedLoss(nn.CrossEntropyLoss):
    def forward(self, pred, label, valid_len):
        weights = torch.ones_like(label)
        weights = sequence_mask(weights, valid_len)  # 生成掩码
        loss = super().forward(pred, label)
        return (loss * weights).mean()              # 屏蔽填充位置
```

**公式说明：**
损失函数计算时，每个位置的损失乘以权重（0或1）。

## 5. 训练技巧：强制教学与梯度裁剪

### 5.1 强制教学（Teacher Forcing）
训练时，解码器的输入使用真实标签而非预测结果，避免错误累积。

**示例：**
- 输入：`<bos>, Ils, regardent`
- 而非：`<bos>, 预测词1, 预测词2`

![强制教学示意图](https://zh-v2.d2l.ai/_images/seq2seq-teacher-forcing.svg)

### 5.2 梯度裁剪
防止梯度爆炸，设定阈值裁剪梯度。

```python
def grad_clipping(net, theta):
    params = [p for p in net.parameters() if p.requires_grad]
    norm = torch.sqrt(sum((p.grad**2).sum() for p in params))
    if norm > theta:
        for p in params:
            p.grad[:] *= theta / norm
```

## 6. 预测：逐步生成输出

预测时解码器每一步的输入是上一步的预测结果：

```python
def predict_seq2seq(net, src_sentence):
    # 编码输入
    enc_X = encode(src_sentence)
    state = encoder(enc_X)
    # 初始输入是<bos>
    dec_X = torch.tensor([[tgt_vocab['<bos>']]])
    output_seq = []
    for _ in range(max_len):
        Y, state = decoder(dec_X, state)
        dec_X = Y.argmax(dim=2)  # 选择概率最高的词
        pred = dec_X.item()
        if pred == '<eos>':
            break
        output_seq.append(pred)
    return output_seq
```

## 7. 评估翻译质量：BLEU分数

BLEU通过比较预测与参考翻译的n-gram相似度打分：

$$ BLEU = \exp\left(\min(0, 1 - \frac{len_{label}}{len_{pred}}\right) \prod_{n=1}^k p_n^{1/2^n} $$

其中$p_n$是n-gram精确度。

**示例：**
- 参考翻译：`Ils regardent .`
- 预测：`Ils entendent .`
- 1-gram精确度：1/2（"Ils"正确）
- 2-gram精确度：0/1
- BLEU得分：$\exp(1 - 3/2) \times (0.5^{0.5} \times 0^{0.25}) = 0$

**LaTeX公式：**
```
$$ BLEU = \exp\left(\min(0, 1 - \frac{len_{label}}{len_{pred}}\right) \prod_{n=1}^k p_n^{1/2^n} $$
```

## 8. 小结

- **编码器-解码器结构** 是处理序列转换任务的基础框架。
- **遮蔽损失** 和 **强制教学** 是训练时的关键技巧。
- **BLEU分数** 客观评估生成质量。
- 实际应用时，可以使用注意力机制（后续章节）进一步提升性能。

通过本篇，你已经掌握了机器翻译等seq2seq任务的核心原理。试着用PyTorch实现一个简单的翻译模型吧！